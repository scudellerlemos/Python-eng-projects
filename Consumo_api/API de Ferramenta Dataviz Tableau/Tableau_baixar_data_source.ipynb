{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando software\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "Variaveis e credenciais configuradas\n",
      ".......\n",
      ".......\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "#importar libs e variaveis/listas\n",
    "import tableauserverclient as TSC\n",
    "import pandas as pd\n",
    "import pantab as pt\n",
    "import zipfile\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from google.cloud import storage,bigquery\n",
    "import pandas_gbq\n",
    "import tableauhyperapi as thyapi\n",
    "\n",
    "\n",
    "#funciona só no WINDOWS com as devidas chaves escondidas no arquivo \".env\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#####################################\n",
    "\n",
    "\n",
    "##############TABLEAU SERVER##################################################################\n",
    "#chaves do tableau server\n",
    "USERNAME = os.environ[\"USUARIO\"]\n",
    "PASSWORD = os.environ[\"SENHA\"]\n",
    "SITE_NAME =  os.environ[\"SITE_NAME\"]\n",
    "\n",
    "#chaves e nomes do datasource\n",
    "ID_datasource= [\"4e2c1d2d-2a94-43a6-8b66-43dcd500c0e9\"]\n",
    "NAME_datasource = [\"Seguranca_de_linha\"]\n",
    "#############################################################################################\n",
    "\n",
    "##############ARQUIVO DE LOGION GCP#########################################################\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'ServiceKey_GoogleCloud.json'\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "##############GOOLE CLOUD STORAGE############################################################\n",
    "#nome do bucket no Google Cloud Storage\n",
    "#NAME_bucket = \"data_teste_bucket\"\n",
    "\n",
    "#arquivo em PARQUET dataframe pandas\n",
    "#NAME_dataset = NAME_datasource[0]+\".parquet\"\n",
    "#############################################################################################\n",
    "\n",
    "##############!!GCP - BIG QUERY!!!###########################################################\n",
    "\n",
    "#ID do Projecto no Big query\n",
    "ID_PROJECT_BQ = \"tableau-api-378220\"\n",
    "\n",
    "#Nome do conjunto de dados e tabela do BQ\n",
    "DATASET = \"Teste\"\n",
    "Table1 = NAME_datasource[0]\n",
    "\n",
    "DATASET_DATANAME = DATASET+\".\"+Table1\n",
    "\n",
    "print(\"Iniciando software\")\n",
    "print(\".......\")\n",
    "print(\".......\")\n",
    "print(\".......\")\n",
    "print(\"Variaveis e credenciais configuradas\")\n",
    "print(\".......\")\n",
    "print(\".......\")\n",
    "print(\".......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando login no Tableau server!\n"
     ]
    }
   ],
   "source": [
    "#login tableau server\n",
    "tableau_auth = TSC.TableauAuth(USERNAME, PASSWORD, '')\n",
    "server = TSC.Server(SITE_NAME)\n",
    "server.use_highest_version()\n",
    "print(\"Realizando login no Tableau server!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando download dos arquivos\n"
     ]
    }
   ],
   "source": [
    "#baixar extrações \n",
    "print(\"Iniciando download dos arquivos\")\n",
    "with server.auth.sign_in(tableau_auth):\n",
    "     for ID in range(0,len(ID_datasource)):\n",
    "          while zipfile.is_zipfile(NAME_datasource[ID]+'.tdsx') ==False:\n",
    "               server.datasources.download(datasource_id= ID_datasource[ID], include_extract=True)\n",
    "               if zipfile.is_zipfile(NAME_datasource[ID]+'.tdsx') ==False:\n",
    "                    print(\"Arquivo corrompido... baixando novamente...\")\n",
    "               else:\n",
    "                    print(\"Arquivo baixado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extrair arquivos e extrair para Hyper na pasta \"data\"\n",
    "for NAME in NAME_datasource:\n",
    "    with zipfile.ZipFile(NAME +'.tdsx', 'r') as zip_ref:\n",
    "        zip_ref.extractall('')\n",
    "\n",
    "print(\"Arquivos extraidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with thyapi.HyperProcess(telemetry=thyapi.Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU ) as hyper:\n",
    "    with thyapi.Connection(endpoint=hyper.endpoint, database='Data\\Extracts\\Seguranca_de_linha.hyper') as connection:\n",
    "         table_names = connection.catalog.get_table_names(schema=\"Extract\")\n",
    "         for table in table_names:\n",
    "            table_definition = connection.catalog.get_table_definition(name=table)\n",
    "            print(f\"Table {table.name} has qualified name: {table}\")\n",
    "  \n",
    "            for column in table_definition.columns:\n",
    "                print(f\"Column {column.name} has type={column.type} and nullability={column.nullability}\")\n",
    "\n",
    "            print(\"\")\n",
    "            # Print all rows from the \"Extract\".\"Extract\" table.\n",
    "            table_name = thyapi.TableName(\"Extract\", \"Extract\")\n",
    "            ###print(f\"These are all rows in the table {table_name}:\")\n",
    "            # `execute_list_query` executes a SQL query and returns the result as list of rows of data,\n",
    "            # each represented by a list of objects.\n",
    "\n",
    "            max_rows = connection.execute_list_query(query=f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "            print(f\"This table has {max_rows[0][0]} rows \" )\n",
    "            ###print(\"The connection to the Hyper file has been closed.\")\n",
    "            rows_in_list= connection.execute_list_query(query=f\"SELECT * FROM {table_name} LIMIT 1000\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega os datasets, conversão de .hyper para dataset pandas\n",
    "dataset = pt.frame_from_hyper('Data/Extracts/'+ NAME_datasource[0] +'.hyper',table=table_name,use_float_na=True )\n",
    "dataset.to_csv(\"teste_base_seguranca_linha.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tratamento de excessão pois não aceita \"espaços\" como nome de coluna\n",
    "list_columns = dataset.columns\n",
    "list_columns = [re.sub(r' ', '_', file) for file in list_columns]\n",
    "dataset.columns = list_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "from google.cloud import bigquery\n",
    "auth.authenticate_user()\n",
    "client = bigquery.Client(project='self-service-saude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleta todos os arquivos [tds,tdsx] e pastas da extração\n",
    "shutil.rmtree('Data')\n",
    "for arquivo in range (0, len(NAME_datasource)):\n",
    "    os.remove(NAME_datasource[arquivo]+'.tdsx')\n",
    "    os.remove(NAME_datasource[arquivo]+'.tds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autenticação GOOGLE CLOUD\n",
    "#storage_client = storage.Client()\n",
    "\n",
    "#autenticação no bucket escolhido do Google Cloud Storage e BigQuery\n",
    "#my_bucket = storage_client.get_bucket(NAME_bucket) \n",
    "bigquery_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######função para subir arquivo no Google Cloud Storage\n",
    "#def upload_bucket(blob_name, file_path, bucket_name):\n",
    "#    try:\n",
    "#        bucket = storage_client.get_bucket(bucket_name)\n",
    "#        blob = bucket.blob(blob_name)\n",
    "#        blob.upload_from_filename(file_path)\n",
    "#        return True\n",
    "#    except Exception as e:\n",
    "#        print(e)\n",
    "#        return False\n",
    "######Função para download de arquivo no Goole Cloud Storage    \n",
    "#def download_file_bucket(blob_name, file_path, bucket_name):\n",
    "#    try:\n",
    "#        bucket = storage_client.get_bucket(bucket_name)\n",
    "#        blob = bucket.blob(blob_name)\n",
    "#        with open(file_path, 'wb') as f:\n",
    "#            storage_client.download_blob_to_file(blob,f)\n",
    "#        return True\n",
    "#    except Exception as e:\n",
    "#        print(e)\n",
    "#        return False\n",
    "\n",
    "#### Download Exemplo\n",
    "# download_file_bucket(NAME_dataset,os.path.join(os.getcwd(),NAME_dataset),NAME_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload dos arquivos ao Bucket\n",
    "#upload_bucket(NAME_dataset,os.path.abspath(NAME_dataset),NAME_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remoção  dos arquivos pandas em .parquet\n",
    "#os.remove(NAME_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para subir o dataset pandas direto para o Bigquery\n",
    "def load_bq(df,dataset):\n",
    "    pandas_gbq.to_gbq(\n",
    "        df, dataset,project_id =ID_PROJECT_BQ,if_exists=\"replace\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload das extrações para o BigQUERY\n",
    "load_bq(dataset,DATASET_DATANAME)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a0efc7ed7210c8daa9aadd584bbae766f571d407ba1506112b0e572af6a4d67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
